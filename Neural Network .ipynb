{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structured-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from numba import jit, prange\n",
    "import os\n",
    "\n",
    "# 是否有 GPU，需要配置 PyTorch GPU 环境\n",
    "has_gpu = torch.cuda.is_available()\n",
    "\n",
    "@jit\n",
    "def im2col(data, kernel_size, stride):\n",
    "    batch_size, input_channel, height, width = data.shape\n",
    "    # 根据公式计算 feature map 的大小\n",
    "    feature_height = int((height-kernel_size)/stride)+1\n",
    "    feature_width = int((width-kernel_size)/stride)+1 \n",
    "    \n",
    "    # 初始化展平矩阵的大小, B*(H*W)*(C*K*K)\n",
    "    col_data = np.zeros((batch_size, feature_height*feature_width, kernel_size*kernel_size*input_channel), dtype=np.float32)\n",
    "    \n",
    "    # 卷积的滑窗\n",
    "    for n in prange(batch_size):\n",
    "        for i in prange(feature_height):\n",
    "            for j in prange(feature_width):\n",
    "                # 将该窗口的数据展平保存\n",
    "                col_data[n, i*feature_width+j, :] = np.ravel(data[n, :, i*stride: i*stride+kernel_size, j*stride: j*stride+kernel_size])\n",
    "            \n",
    "    # 返回展平后的结果，和 feature map 的高、宽\n",
    "    return col_data, feature_height, feature_width\n",
    "\n",
    "def matmul(input1, input2):\n",
    "    assert input1.shape[0]==input2.shape[0], '必须相等'\n",
    "    if has_gpu:\n",
    "        grad = torch.sum(torch.einsum('ijk,ikl->ijl', (torch.from_numpy(input1).cuda(), torch.from_numpy(input2).cuda())), dim=0).cpu().numpy()\n",
    "    else:\n",
    "        grad = np.sum(np.einsum('ijk,ikl->ijl', input1, input2), axis=0)\n",
    "    return grad\n",
    "\n",
    "# 计算输入的梯度，将展开的梯度还原\n",
    "def col2im(col_data, top_grad, weight, shape):\n",
    "    # 参数\n",
    "    batch_size, input_channel, width, height, feature_height, feature_width, kernel_size, stride= shape\n",
    "    # 初始化原始梯度，和输入数据一样\n",
    "    grad = np.zeros((batch_size, input_channel, width, height), dtype=np.float32)\n",
    "    # 对每个样本的计算梯度\n",
    "    if has_gpu:\n",
    "        grad_one_ = torch.matmul(torch.from_numpy(top_grad).cuda(), torch.from_numpy(weight).cuda()).cpu().numpy()\n",
    "    else:\n",
    "        grad_one_ = np.matmul(top_grad, weight)\n",
    "    for n in prange(batch_size):\n",
    "        for i in prange(feature_height):\n",
    "            for j in prange(feature_width):\n",
    "                # 每个样本的梯度累加\n",
    "                # 被展开的梯度，还原成原始数据的梯度\n",
    "                grad[n, :, i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size] += np.reshape(grad_one_[n, i*feature_width+j, :], (input_channel, kernel_size, kernel_size))\n",
    "    return grad\n",
    "\n",
    "class conv2d(object):\n",
    "    '''\n",
    "    output_channel: 该层卷积核的数量\n",
    "    input_channel: 输入数据的通道数，例如灰度图为 1，彩色图为 3，又或者上一层的卷积数量为 12，那下一层卷积的输入通道为 12\n",
    "    kernel_size: 卷积核大小\n",
    "    stride: 卷积移动的步长\n",
    "    padding: 补齐\n",
    "    '''\n",
    "    def __init__(self, input_channel, output_channel, kernel_size, stride=1, padding=0):\n",
    "        # 初始化网络权重，卷积核数量*输入通道数*卷积核大小*卷积核大小，一个卷积核应为输入通道数*卷积核大小*卷积核大小，每一层有多个卷积核\n",
    "        # self.weight = (np.random.randn(output_channel, input_channel, kernel_size, kernel_size)*0.01).astype(np.float32)\n",
    "        self.weight = copy.deepcopy(torch.nn.Conv2d(input_channel, output_channel, kernel_size, stride).weight.data.numpy())\n",
    "        # 偏置项，每一个卷积核都对应一个偏置项，y=xW+b\n",
    "        self.bias = np.zeros((output_channel), dtype=np.float32)\n",
    "        # 保存各项参数\n",
    "        self.output_channel = output_channel\n",
    "        self.input_channel = input_channel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # 输入数据各个维度代表的含义: batch_size, input_channel, height, width\n",
    "        batch_size, input_channel, height, width = input.shape\n",
    "        # 防止一些错误的调用\n",
    "        assert len(input.shape)==4, '输入必须是四维的，batch_size*channel*width*height'\n",
    "        # 上一层的卷积核数量必须等于这一层的输入通道数\n",
    "        assert input_channel==self.input_channel, '网络输入通道数必须与网络定义的一致'\n",
    "        \n",
    "        # 对输入数据进行补齐\n",
    "        input = np.pad(input, [(0, ), (0, ), (self.padding, ), (self.padding, )], 'constant', constant_values=0)\n",
    "        \n",
    "        # 保存输入数据，后续需要计算梯度\n",
    "        self.data = input\n",
    "        \n",
    "        # 展开数据\n",
    "        self.col_data, self.feature_height, self.feature_width = im2col(input, self.kernel_size, self.stride)\n",
    "        # 计算输出，是否有 GPU，如果有则使用 GPU 计算\n",
    "        if has_gpu:\n",
    "            feature_maps = torch.matmul(torch.from_numpy(self.col_data).cuda(), torch.from_numpy(self.weight.reshape(self.output_channel, -1).T).cuda()) + torch.from_numpy(self.bias).cuda().unsqueeze(0)\n",
    "            feature_maps = torch.reshape(feature_maps, (batch_size, self.feature_height, self.feature_width, self.output_channel)).permute(0, 3, 1, 2).cpu().numpy()\n",
    "        else:\n",
    "            # 计算输出之后同时 reshape, transpose 还原成本来应该的输出大小\n",
    "            # feature_maps = np.dot(self.col_data, self.weight.reshape(self.output_channel, -1).T) + self.bias[np.newaxis, :]\n",
    "            feature_maps = np.matmul(self.col_data, self.weight.reshape(self.output_channel, -1).T) + self.bias[np.newaxis, :]\n",
    "            feature_maps = np.reshape(feature_maps, (batch_size, self.feature_height, self.feature_width, self.output_channel)).transpose(0, 3, 1, 2)\n",
    "        return feature_maps\n",
    "    \n",
    "    '''\n",
    "    top_grad: 上一层的梯度，维度为 batch_size, output_channel, feature_height, feature_width，与该层输出一致\n",
    "    '''\n",
    "    def backward(self, top_grad, lr):\n",
    "        batch_size, output_channel, feature_height, feature_width = top_grad.shape\n",
    "        # 将梯度展开\n",
    "        top_grad = top_grad.transpose((0, 2, 3, 1)).reshape(batch_size, feature_height*feature_width, output_channel)\n",
    "        # 计算权重、偏置的梯度\n",
    "        self.grad_w = matmul(self.col_data.transpose(0, 2, 1), top_grad).T.reshape(output_channel, self.input_channel, self.kernel_size, self.kernel_size)\n",
    "        self.grad_b = np.sum(top_grad, axis=(0, 1))\n",
    "        # 各项参数\n",
    "        shape = list(self.data.shape) + [self.feature_height, self.feature_width, self.kernel_size, self.stride]\n",
    "        # 计算输入的梯度\n",
    "        self.grad = col2im(self.col_data, top_grad, self.weight.reshape(self.output_channel, -1), shape)    \n",
    "        # 更新参数\n",
    "        self.weight -= lr*self.grad_w\n",
    "        self.bias -= lr*self.grad_b\n",
    "        \n",
    "        \n",
    "class MaxPool2d(object):\n",
    "    \n",
    "    # 各项参数\n",
    "    def __init__(self, kernel_size, stride, padding=0):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    # 前向传播\n",
    "    def forward(self, input):\n",
    "        # input: batch_size, input channel, height, width\n",
    "        assert len(input.shape)==4, '输入必须是四维的，batch_size*channel*width*height'\n",
    "        batch_size, input_channel, height, width = input.shape\n",
    "        \n",
    "        # 计算输出的大小，和卷积层一样\n",
    "        feature_height = int((height-self.kernel_size+2*self.padding)/self.stride)+1\n",
    "        feature_width = int((width-self.kernel_size+2*self.padding)/self.stride)+1 \n",
    "        # 初始化输出矩阵\n",
    "        pooled_feature_map = np.zeros((batch_size, input_channel, feature_height, feature_width), dtype=np.float32)\n",
    "        # 按设置的 padding 补齐\n",
    "        input = np.pad(input, [(0, ), (0, ), (self.padding, ), (self.padding, )], 'constant', constant_values=0)\n",
    "        self.data = input\n",
    "        # 窗口滑动\n",
    "        for i in range(feature_height):\n",
    "            for j in range(feature_width):\n",
    "                # 最大池化\n",
    "                pooled_feature_map[:, :, i, j] = np.max(input[:, :, i*self.stride: i*self.stride+self.kernel_size, j*self.stride: j*self.stride+self.kernel_size], axis=(2, 3))\n",
    "        return pooled_feature_map\n",
    "    \n",
    "    # 后向传播\n",
    "    def backward(self, top_grad):\n",
    "        # 输入数据梯度\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float32)\n",
    "        # 上一层梯度的维度\n",
    "        batch_size, output_channel, feature_height, feature_width = top_grad.shape\n",
    "        \n",
    "        # 遍历输出 feature map 的每一个点\n",
    "        for n in range(batch_size):\n",
    "            for i in range(feature_height):\n",
    "                for j in range(feature_width):        \n",
    "                    # 取出这一块的数据\n",
    "                    one_kernel_data = self.data[n, :, i*self.stride: i*self.stride+self.kernel_size, j*self.stride: j*self.stride+self.kernel_size]\n",
    "                    # 下面三行代码都是获得最大值点的坐标\n",
    "                    # 将一个矩形展开\n",
    "                    channels = one_kernel_data.shape[0]\n",
    "                    one_kernel_data = np.reshape(one_kernel_data, (one_kernel_data.shape[0], -1))\n",
    "                    # 获取该矩形最大值所在的坐标\n",
    "                    one_kernel_featuremap_argmax = np.argmax(one_kernel_data, axis=1)\n",
    "                    # np.unravel_index 将被展开的坐标换算成对应的矩形内的坐标\n",
    "                    argmax1, argmax2 = np.unravel_index(one_kernel_featuremap_argmax, (self.kernel_size, self.kernel_size))\n",
    "                    # 误差将会被分散到最大值所在的点上\n",
    "                    self.grad[n, :, i*self.stride: i*self.stride+self.kernel_size, j*self.stride: j*self.stride+self.kernel_size][np.arange(channels), argmax1, argmax2] += top_grad[n, :, i, j]\n",
    "        # 去除 padding 的影响\n",
    "        self.grad = self.grad[:, :, self.padding: self.grad.shape[2]-self.padding, self.padding: self.grad.shape[3]-self.padding]\n",
    "        \n",
    "        \n",
    "import struct\n",
    "\n",
    "# 读取 mnist 数据集\n",
    "def read_mnist(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "\n",
    "# softmax 函数\n",
    "def softmax(input):\n",
    "    exp_value = np.exp(input) #首先计算指数\n",
    "    output = exp_value/np.sum(exp_value, axis=1)[:, np.newaxis] # 然后按行标准化\n",
    "    return output\n",
    "\n",
    "# 交叉熵损失函数\n",
    "class CrossEntropyLossLayer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input, labels):\n",
    "        # 做一些防止误用的措施，输入数据必须是二维的，且标签和数据必须维度一致\n",
    "        assert len(input.shape)==2, '输入的数据必须是一个二维矩阵'\n",
    "        assert len(labels.shape)==2, '输入的标签必须是独热编码'\n",
    "        assert labels.shape==input.shape, '数据和标签数量必须一致'\n",
    "        self.data = input\n",
    "        self.labels = labels\n",
    "        self.prob = np.clip(softmax(input), 1e-9, 1.0) #在取对数时不能为 0，所以用极小数代替 0\n",
    "        loss = -np.sum(np.multiply(self.labels, np.log(self.prob)))/self.labels.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = (self.prob - self.labels)/self.labels.shape[0] # 根据公式计算梯度\n",
    "        \n",
    "# 学习率衰减\n",
    "class lr_scheduler(object):\n",
    "    def __init__(self, base_lr, step_size, deacy_factor=0.1):\n",
    "        self.base_lr = base_lr # 最初的学习率\n",
    "        self.deacy_factor = deacy_factor # 学习率衰减因子\n",
    "        self.step_count = 0 # 当前的迭代次数\n",
    "        self.lr = base_lr # 当前学习率\n",
    "        self.step_size = step_size # 步长\n",
    "        \n",
    "    def step(self, step_count=1): # 默认 1 次\n",
    "        self.step_count += step_count\n",
    "    \n",
    "    def get_lr(self):\n",
    "        self.lr = self.base_lr*(self.deacy_factor**(self.step_count//self.step_size))\n",
    "        return self.lr\n",
    "\n",
    "class ReLU(object):\n",
    "    def forward(self, input):\n",
    "        self.data = input\n",
    "        # 按照公式实现 \n",
    "        return np.maximum(0, input)\n",
    "    def backward(self, top_grad):\n",
    "        if has_gpu:\n",
    "            self.grad = (((torch.from_numpy(self.data).cuda()>0).float())*torch.from_numpy(top_grad).cuda()).cpu().numpy()\n",
    "        else:\n",
    "            self.grad = (self.data>0)*top_grad\n",
    "        # relu 没有需要更新的参数\n",
    "        \n",
    "class Linear(object):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        # 初始化权重和偏置的维度，高斯初始化权重，零初始化偏置\n",
    "        # self.weight = np.random.randn(D_in, D_out).astype(np.float32)*0.01\n",
    "        self.weight = copy.deepcopy(torch.nn.Linear(D_in, D_out).weight.data.numpy().transpose((1, 0)))\n",
    "#         self.weight = np.random.randn(D_in, D_out)/np.sqrt(D_in).astype(np.float32)\n",
    "        self.bias = np.zeros((1, D_out), dtype=np.float32)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 前行传播保存输入数据，并做线性分类\n",
    "        self.data = input\n",
    "        if has_gpu:\n",
    "            output = (torch.matmul(torch.from_numpy(self.data).cuda(), torch.from_numpy(self.weight).cuda())+torch.from_numpy(self.bias).cuda()).cpu().numpy()\n",
    "        else:\n",
    "            output = np.dot(self.data, self.weight)+self.bias\n",
    "        return output\n",
    "\n",
    "    def backward(self, top_grad, lr):\n",
    "        # 后向传播计算梯度，前面已经介绍了如何关于输入计算梯度\n",
    "        if has_gpu:\n",
    "            self.grad = (torch.matmul(torch.from_numpy(top_grad).cuda(), torch.from_numpy(self.weight.T).cuda())).cpu().numpy()\n",
    "            grad_w = torch.matmul(torch.from_numpy(self.data.T).cuda(), torch.from_numpy(top_grad).cuda())\n",
    "        else:\n",
    "            self.grad = np.dot(top_grad, self.weight.T)\n",
    "            grad_w = np.dot(self.data.T, top_grad)\n",
    "        grad_b = np.mean(top_grad, axis=0)\n",
    "        # 更新参数，最后一项为损失关于当前权重的梯度\n",
    "        self.weight -= lr*grad_w\n",
    "        # y=xW+b 关于 b 的偏导为 1，已经介绍过了\n",
    "        self.bias -= lr*grad_b\n",
    "\n",
    "class Dataloader(object):\n",
    "    def __init__(self, data, labels, batch_size, shuffle=True):\n",
    "        # 初始数据和标签\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        # 批量大小\n",
    "        self.batch_size = batch_size\n",
    "        # 是否打乱，默认打乱数据集，只针对训练集\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据下标返回数据\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        datasize = self.data.shape[0]\n",
    "        # 生成迭代序列\n",
    "        data_seq = np.arange(datasize)\n",
    "        if self.shuffle:\n",
    "            # 打乱迭代序列\n",
    "            np.random.shuffle(data_seq)\n",
    "        # 生成的是 Batch 序列\n",
    "        interval_list = np.append(np.arange(0, datasize, self.batch_size), datasize)\n",
    "        for index in range(interval_list.shape[0]-1):\n",
    "            s = data_seq[interval_list[index]:interval_list[index+1]]\n",
    "            # 返回 batch 的数据\n",
    "            yield self.data[s], self.labels[s]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集长度\n",
    "        return self.data.shape[0]\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import copy\n",
    "\n",
    "# Net 为网络结构，需要定义 backward 和 forward 操作\n",
    "def train_and_test(loss_layer, net, scheduler, max_iter, train_dataloader, test_dataloader, batch_size):\n",
    "    test_loss_list, train_loss_list, train_acc_list, test_acc_list = [], [], [], []\n",
    "    best_net = None\n",
    "    # 最高准确度，和对应权重\n",
    "    best_acc = -float('inf')\n",
    "    for epoch in range(max_iter):\n",
    "        # 训练\n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        with tqdm_notebook(total=len(train_dataloader)//batch_size+1) as pbar:\n",
    "            for data, labels in train_dataloader:\n",
    "                # 前向输出概率\n",
    "                train_pred = net.forward(data)\n",
    "\n",
    "                # 计算准确度\n",
    "                pred_labels = np.argmax(train_pred, axis=1)\n",
    "                real_labels = np.argmax(labels, axis=1)\n",
    "                correct += np.sum(pred_labels==real_labels)\n",
    "\n",
    "                # 前向输出损失\n",
    "                loss = loss_layer.forward(train_pred, labels)\n",
    "                total_loss += loss*data.shape[0]\n",
    "\n",
    "                # 反向更新参数\n",
    "                loss_layer.backward()\n",
    "                # print(epoch, loss, correct)\n",
    "                net.backward(loss_layer.grad, scheduler.get_lr())\n",
    "                pbar.update(1)\n",
    "            \n",
    "        acc = correct/len(train_dataloader)\n",
    "        print('Epoch {}/{}: train accuracy, {},  train loss: {}'.format(epoch+1, max_iter, acc, total_loss/len(train_dataloader)))\n",
    "        train_acc_list.append(acc)\n",
    "        train_loss_list.append(total_loss/len(train_dataloader))\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 测试\n",
    "        correct = 0\n",
    "        total_loss = 0\n",
    "        for data, labels in test_dataloader:\n",
    "            # 前向输出概率\n",
    "            test_pred = net.forward(data)\n",
    "\n",
    "            # 前向输出损失\n",
    "            loss = loss_layer.forward(test_pred, labels)\n",
    "            total_loss += loss*data.shape[0]\n",
    "\n",
    "            # 计算准确度\n",
    "            pred_labels = np.argmax(test_pred, axis=1)\n",
    "            real_labels = np.argmax(labels, axis=1)\n",
    "            correct += np.sum(pred_labels==real_labels)\n",
    "        acc = correct/len(test_dataloader)\n",
    "        test_acc_list.append(acc)\n",
    "        test_loss_list.append(total_loss/len(test_dataloader))\n",
    "        print('Epoch {}/{}: test accuracy, {},  test loss: {}'.format(epoch+1, max_iter, acc, total_loss/len(test_dataloader)))\n",
    "\n",
    "        if acc > best_acc: \n",
    "            best_acc = acc\n",
    "            best_net = copy.deepcopy(net)\n",
    "    return test_loss_list, train_loss_list, train_acc_list, test_acc_list, best_net\n",
    "\n",
    "\n",
    "class LeNet(object):\n",
    "    def __init__(self):\n",
    "        self.conv1 = conv2d(1, 20, 5, 1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2d(2, 2)\n",
    "        self.conv2 = conv2d(20, 50, 5, 1)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2d(2, 2)\n",
    "        self.fc1 = Linear(800, 500)\n",
    "        self.relu3 = ReLU()\n",
    "        self.fc2 = Linear(500, 10)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.relu1.forward(self.conv1.forward(input))\n",
    "        input = self.pool1.forward(input)\n",
    "        input = self.relu2.forward(self.conv2.forward(input))\n",
    "        input = self.pool2.forward(input)\n",
    "        # 展开\n",
    "        self.flatten_shape = input.shape\n",
    "        input = np.reshape(input, (input.shape[0], -1))\n",
    "        input = self.relu3.forward(self.fc1.forward(input))\n",
    "        output = self.fc2.forward(input)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, top_grad, lr):\n",
    "        self.fc2.backward(top_grad, lr)\n",
    "        self.relu3.backward(self.fc2.grad)\n",
    "        self.fc1.backward(self.relu3.grad, lr)\n",
    "        unflattened_grad = np.reshape(self.fc1.grad, self.flatten_shape)\n",
    "        self.pool2.backward(unflattened_grad)\n",
    "        self.relu2.backward(self.pool2.grad)\n",
    "        self.conv2.backward(self.relu2.grad, lr)\n",
    "        self.pool1.backward(self.conv2.grad)\n",
    "        self.relu1.backward(self.pool1.grad)\n",
    "        self.conv1.backward(self.relu1.grad, lr)\n",
    "\n",
    "# 读取 cifar10 数据集，传入 cifar10 所在目录\n",
    "def read_cifar10(filepath):\n",
    "    import pickle\n",
    "    # 读取压缩文件\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            file_dict = pickle.load(fo, encoding='bytes')\n",
    "        return file_dict\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    # 读取训练数据\n",
    "    for i in range(1, 6):\n",
    "        file_dict = unpickle(os.path.join(filepath, 'data_batch_%d'%i))\n",
    "        train_data.append(file_dict[b'data'])\n",
    "        train_labels += file_dict[b'labels']\n",
    "    train_data = np.concatenate(train_data).astype(np.float32)\n",
    "    # 读取测试数据\n",
    "    file_dict = unpickle(os.path.join(filepath, 'test_batch'))\n",
    "    test_data, test_labels = file_dict[b'data'].astype(np.float32), file_dict[b'labels']\n",
    "\n",
    "    return train_data.reshape(train_data.shape[0], 3, 32, 32), np.array(train_labels), test_data.reshape(test_data.shape[0], 3, 32, 32), np.array(test_labels)\n",
    "\n",
    "\n",
    "class Cifar10Net(object):\n",
    "    def __init__(self):\n",
    "        self.conv1 = conv2d(3, 20, 5, 1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2d(2, 2)\n",
    "        self.conv2 = conv2d(20, 50, 5, 1)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2d(2, 2)\n",
    "        self.fc1 = Linear(1250, 500)\n",
    "        self.relu3 = ReLU()\n",
    "        self.fc2 = Linear(500, 10)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.relu1.forward(self.conv1.forward(input))\n",
    "        input = self.pool1.forward(input)\n",
    "        input = self.relu2.forward(self.conv2.forward(input))\n",
    "        input = self.pool2.forward(input)\n",
    "        # 展开\n",
    "        self.flatten_shape = input.shape\n",
    "        input = np.reshape(input, (input.shape[0], -1))\n",
    "        input = self.relu3.forward(self.fc1.forward(input))\n",
    "        output = self.fc2.forward(input)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, top_grad, lr):\n",
    "        self.fc2.backward(top_grad, lr)\n",
    "        self.relu3.backward(self.fc2.grad)\n",
    "        self.fc1.backward(self.relu3.grad, lr)\n",
    "        unflattened_grad = np.reshape(self.fc1.grad, self.flatten_shape)\n",
    "        self.pool2.backward(unflattened_grad)\n",
    "        self.relu2.backward(self.pool2.grad)\n",
    "        self.conv2.backward(self.relu2.grad, lr)\n",
    "        self.pool1.backward(self.conv2.grad)\n",
    "        self.relu1.backward(self.pool1.grad)\n",
    "        self.conv1.backward(self.relu1.grad, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interstate-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SimpleNet 实现\n",
    "各项参数如下:\n",
    "\n",
    "输入数据为 $B110*10，，B$ 为批量大小，即输入通道数为 1。\n",
    "第一层卷积层个数为 20，卷积核大小为 5，步长为 1。\n",
    "ReLU 激活层。\n",
    "步长为 2，核大小为 2 的最大池化层。\n",
    "Flatten 之后接入一层线性层、ReLU 激活的模块，线性层输入为 180，输出为 100。\n",
    "最后一层进行分类，无 ReLU，输出为 10.\n",
    "**提示**：请注意完成 forward 和 backward 两个函数，并注意池化层到线性层数据需要被展开。\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleNet(object):\n",
    "    def __init__(self):\n",
    "        # 代码开始 ### (≈ 6 行代码)\n",
    "        self.conv1 = conv2d(1,20,5,1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2d(2,2)\n",
    "        self.fc1 = Linear(180,100)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc2 = Linear(100,10)\n",
    "        ### 代码结束 ###\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "        input -- 输入数据\n",
    "        返回:\n",
    "        output -- 神经网络输出数据\n",
    "        \"\"\"\n",
    "        # 代码开始 ### (≈ 10 行代码)\n",
    "        input = self.relu1.forward(self.conv1.forward(input))\n",
    "        input = self.pool1.forward(input)\n",
    "        self.flatten_shape = input.shape\n",
    "        input = np.reshape(input,(input.shape[0],-1))\n",
    "        input = self.relu2.forward(self.fc1.forward(input))\n",
    "        output = self.fc2.forward(input)\n",
    "        return output\n",
    "        ### 代码结束 ###\n",
    "\n",
    "    def backward(self, top_grad, lr):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "        top_grad -- 上一层梯度\n",
    "        lr -- 初始学习率\n",
    "        返回:\n",
    "        无\n",
    "        \"\"\"\n",
    "        # 代码开始 ### (≈ 7 行代码)\n",
    "        self.fc2.backward(top_grad, lr)\n",
    "        self.relu2.backward(self.fc2.grad)\n",
    "        self.fc1.backward(self.relu2.grad, lr)\n",
    "        unflattened_grad = np.reshape(self.fc1.grad, self.flatten_shape)\n",
    "        self.pool1.backward(unflattened_grad)\n",
    "        self.relu1.backward(self.pool1.grad)\n",
    "        self.conv1.backward(self.relu1.grad, lr)\n",
    "        ### 代码结束 ###\n",
    "        \n",
    "class TorchNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchNet, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 20, 5, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "    \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(180, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #self.flatten_shape = input.shape\n",
    "        #input = np.reshape(input,(input.shape[0],-1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = np.reshape(x,(x.size[0],-1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "familiar-bennett",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.3315865   0.71527897]\n",
      "   [ 0.43302619  1.20303737]]]\n",
      "\n",
      "\n",
      " [[[ 0.11747566 -1.90745689]\n",
      "   [ 0.29294072 -0.47080725]]]]\n",
      "\n",
      "tensor([[[[ 1.3316,  0.7153],\n",
      "          [ 0.4330,  1.2030]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1175, -1.9075],\n",
      "          [ 0.2929, -0.4708]]]])\n",
      "\n",
      "[[-0.1626764  -0.21762127 -0.07795562 -0.0424239  -0.12086755  0.06888997\n",
      "   0.05691363 -0.0281157  -0.4101945   0.046524  ]\n",
      " [-0.12926707 -0.08638534 -0.04449985 -0.10439779 -0.0916423   0.09180234\n",
      "  -0.03483336 -0.14310843 -0.4089186  -0.0713471 ]]\n",
      "\n",
      "tensor([[ 0.3089,  0.1918,  0.2641,  0.3564,  0.1590, -0.0052, -0.0805, -0.2107,\n",
      "          0.0450,  0.0576],\n",
      "        [ 0.3219,  0.2387,  0.1183,  0.4128,  0.1718,  0.0545, -0.0974, -0.1542,\n",
      "          0.1521, -0.1120]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "data1 = np.random.randn(2, 1, 10, 10)\n",
    "data2= torch.tensor(data1,dtype=torch.float32)\n",
    "print(data1[:,:,:2,:2])\n",
    "print()\n",
    "print(data2[:,:,:2,:2])\n",
    "print()\n",
    "\n",
    "net = SimpleNet()\n",
    "torchnet = TorchNet()\n",
    "output1 = net.forward(data1)  # 前向传播\n",
    "print(output1)\n",
    "print()\n",
    "\n",
    "torchnet.train()\n",
    "output2 = torchnet.forward(data2)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acute-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.00066155, -0.01311418],\n",
       "         [ 0.02485082, -0.00623123]]],\n",
       "\n",
       "\n",
       "       [[[-0.00253486,  0.00286818],\n",
       "         [-0.00192729,  0.03077973]]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_grad = np.ones_like(output1)\n",
    "net.backward(top_grad, 0.)  # 反向传播\n",
    "net.conv1.grad[:, :, :2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "innovative-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import random\n",
    "\n",
    "def monte_carlo_pi(nsamples):\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x**2 + y**2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def monte_carlo_pi_numba(nsamples):\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x**2 + y**2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "persistent-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.2 µs ± 133 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Larry Chen\\anaconda3\\envs\\torch\\lib\\site-packages\\numba\\core\\typed_passes.py:327: NumbaPerformanceWarning: \u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-5-f47e102bc9c0>\", line 14:\u001b[0m\n",
      "\u001b[1m@jit(nopython=True, parallel=True)\n",
      "\u001b[1mdef monte_carlo_pi_numba(nsamples):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 µs ± 2.01 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "3.44\n",
      "3.12\n"
     ]
    }
   ],
   "source": [
    "%timeit monte_carlo_pi(100)\n",
    "%timeit monte_carlo_pi_numba(100)\n",
    "a=monte_carlo_pi(100)\n",
    "b=monte_carlo_pi_numba(100)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "happy-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "'利用torch搭建CNN网络'\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, 5, 1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Conv2d(64, 128, 3, 1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # BatchNorm 正则化方法\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Conv2d(128, 256, 2, 1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(256, 256, 2, 1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # BatchNorm 正则化方法\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "    \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1024, 1024),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "actual-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代次数、学习率等\n",
    "batch_size = 120\n",
    "base_lr = 0.1\n",
    "EPOCHS = 20\n",
    "step_size = 8\n",
    "download = True\n",
    "best_acc = -float('inf')\n",
    "    \n",
    "# 加载数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data/', train=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# 定义网络结构和优化器\n",
    "model = Net()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# weight_decay 表明使用权重衰减的系数，不宜过大，一般取小数点四位再慢慢调整\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "residential-nepal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:14<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "Train set: Average loss: 0.0139, Accuracy: 19952/50000 (39.90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.9027, Accuracy: 3697/10000 (36.97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:\n",
      "Train set: Average loss: 0.0105, Accuracy: 27579/50000 (55.16%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.2679, Accuracy: 5440/10000 (54.40%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:\n",
      "Train set: Average loss: 0.0092, Accuracy: 30447/50000 (60.89%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.5004, Accuracy: 4952/10000 (49.52%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:\n",
      "Train set: Average loss: 0.0083, Accuracy: 32552/50000 (65.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.2324, Accuracy: 5817/10000 (58.17%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:\n",
      "Train set: Average loss: 0.0077, Accuracy: 33953/50000 (67.91%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.2219, Accuracy: 5696/10000 (56.96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:\n",
      "Train set: Average loss: 0.0072, Accuracy: 34858/50000 (69.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.0335, Accuracy: 6353/10000 (63.53%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:14<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:\n",
      "Train set: Average loss: 0.0068, Accuracy: 35760/50000 (71.52%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.2724, Accuracy: 5907/10000 (59.07%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:\n",
      "Train set: Average loss: 0.0068, Accuracy: 35924/50000 (71.85%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 3.1689, Accuracy: 2497/10000 (24.97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:\n",
      "Train set: Average loss: 0.0044, Accuracy: 40790/50000 (81.58%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6769, Accuracy: 7682/10000 (76.82%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:14<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:\n",
      "Train set: Average loss: 0.0035, Accuracy: 42801/50000 (85.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6823, Accuracy: 7712/10000 (77.12%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:14<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:\n",
      "Train set: Average loss: 0.0030, Accuracy: 43895/50000 (87.79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7669, Accuracy: 7503/10000 (75.03%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:14<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:\n",
      "Train set: Average loss: 0.0025, Accuracy: 44940/50000 (89.88%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6651, Accuracy: 7841/10000 (78.41%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:15<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:\n",
      "Train set: Average loss: 0.0021, Accuracy: 45832/50000 (91.66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6893, Accuracy: 7790/10000 (77.90%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:17<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:\n",
      "Train set: Average loss: 0.0018, Accuracy: 46650/50000 (93.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7218, Accuracy: 7728/10000 (77.28%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:16<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:\n",
      "Train set: Average loss: 0.0015, Accuracy: 47368/50000 (94.74%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.9061, Accuracy: 7405/10000 (74.05%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:14<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:\n",
      "Train set: Average loss: 0.0012, Accuracy: 47999/50000 (96.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7729, Accuracy: 7738/10000 (77.38%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:17<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:\n",
      "Train set: Average loss: 0.0007, Accuracy: 49258/50000 (98.52%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7378, Accuracy: 7885/10000 (78.85%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:19<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:\n",
      "Train set: Average loss: 0.0006, Accuracy: 49561/50000 (99.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7471, Accuracy: 7874/10000 (78.74%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:20<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:\n",
      "Train set: Average loss: 0.0005, Accuracy: 49659/50000 (99.32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7538, Accuracy: 7880/10000 (78.80%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 417/417 [01:18<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:\n",
      "Train set: Average loss: 0.0005, Accuracy: 49703/50000 (99.41%)\n",
      "Test set: Average loss: 0.7593, Accuracy: 7878/10000 (78.78%)\n",
      "\n",
      "best accuracy:  0.7885\n"
     ]
    }
   ],
   "source": [
    "# 开始迭代\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    # 训练\n",
    "    for data, target in tqdm(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        train_loss += loss.item()\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    exp_lr_scheduler.step()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('Epoch {}/{}:'.format(epoch, EPOCHS))\n",
    "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        train_loss, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # 测试\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct/len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * acc))\n",
    "    if best_acc<acc: best_acc=acc\n",
    "    print()\n",
    "\n",
    "print('best accuracy: ', best_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
