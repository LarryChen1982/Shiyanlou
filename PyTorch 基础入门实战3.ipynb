{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "finnish-forth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (linear): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型的保存与加载\n",
    "'''\n",
    "介绍\n",
    "本实验主要讲解了在不同环境下，如何使用 PyTorch 对模型进行加载的过程。\n",
    "在本实验中我们将学到 torch.save() 、 torch.load() 和 torch.nn.Module().loadstatedict() 的作用以及使用方式。\n",
    "\n",
    "知识点\n",
    "完整模型的保存\n",
    "模型参数的保存\n",
    "模型的加载\n",
    "'''\n",
    "\n",
    "'''\n",
    "模型的保存与加载\n",
    "模型训练的实质就是优化模型中的参数，使模型损失最小的过程。而模型保存其实也有两种方式，\n",
    "一种是直接保存整个模型，另一种就是保存模型的参数。接下来，让我们以一个简单的模型为例子。\n",
    "\n",
    "下面我们建立了一个简单的全连接网络模型：\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# 输入层为6个神经元节点\n",
    "model = Model(n_input_features=6)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accessory-register",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存成功\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "整个模型的保存与加载\n",
    "我们可以将整个模型直接进行保存，使用 torch.save(model, FILE) 即可，其中 model 为模型的变量名，FILE 为想要保存的文件路径。\n",
    "'''\n",
    "# 将文件保存为 model.pth\n",
    "FILE = \"./model/tmpmodel.pth\"\n",
    "torch.save(model, FILE)\n",
    "print(\"保存成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adult-florist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (linear): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#接下来让我们使用 torch.load(FILE) 来对模型进行加载：\n",
    "\n",
    "# 由于模型中已经有了结构和参数，因此我们可以直接用一个新的变量接它即可\n",
    "loaded_model = torch.load(FILE)\n",
    "# 再展示之前，必须需要告诉模型现在在做模型评估，避免模型自动梯度下降\n",
    "loaded_model.eval()\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-homework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存前：\n",
      "Parameter containing:\n",
      "tensor([[ 0.2545,  0.0153, -0.2169, -0.1999, -0.1549,  0.3730]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2678], requires_grad=True)\n",
      "=====================================\n",
      "保存后：\n",
      "Parameter containing:\n",
      "tensor([[ 0.2545,  0.0153, -0.2169, -0.1999, -0.1549,  0.3730]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2678], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#我们可以使用 model.parameters() 查看保存前和保存后的模型参数是否发生变化：\n",
    "# 保存前\n",
    "print(\"保存前：\")\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "print(\"=====================================\")\n",
    "# 加载后\n",
    "print(\"保存后：\")\n",
    "for param in loaded_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brutal-office",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[ 0.2545,  0.0153, -0.2169, -0.1999, -0.1549,  0.3730]])), ('linear.bias', tensor([-0.2678]))])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "模型参数的保存与加载\n",
    "由于模型除了参数之外还存在模型结构等内容，保存整个模型的文件一般都会比只保存模型参数的文件大得多。\n",
    "因此，我们在训练过程中都会选择只保存模型参数。\n",
    "\n",
    "我们可以使用 model.state_dict() 将模型参数转为字典对象，即每层网络结构的参数分开，如下：\n",
    "'''\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imposed-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存成功\n"
     ]
    }
   ],
   "source": [
    "#模型参数的保存，其实就是对上面这种字典对象的保存。我们可以使用 torch.save(model.state_dict(), FILE) 对模型参数进行保存。\n",
    "FILE1 = \"./model/tmpmodel1.pth\"\n",
    "torch.save(model.state_dict(), FILE1)\n",
    "print(\"保存成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "growing-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (linear): Linear(in_features=6, out_features=1, bias=True)\n",
      ")\n",
      "OrderedDict([('linear.weight', tensor([[ 0.2545,  0.0153, -0.2169, -0.1999, -0.1549,  0.3730]])), ('linear.bias', tensor([-0.2678]))])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "由于此时我们只保存了模型的参数，因此在加载模型时，我们需要提前指定模型的网络结构。\n",
    "如果指定的网络结构和我们定义的模型参数不匹配，则会报错。\n",
    "'''\n",
    "# 指定网络结构\n",
    "loaded_model = Model(n_input_features=6)\n",
    "# 加载参数\n",
    "dicts = torch.load(FILE1)\n",
    "loaded_model.load_state_dict(dicts)\n",
    "print(loaded_model)\n",
    "print(loaded_model.state_dict())\n",
    "\n",
    "\n",
    "'''\n",
    "从结果可以看出，loaded_model.load_state_dict(torch.load(FILE)) 可以很好的加载模型的参数。\n",
    "\n",
    "综上，如果我们保存的是整个模型，那么我们就可以直接加载该模型。\n",
    "如果我们保存的仅仅是模型的参数，那么在加载模型之前我们就需要先定义模型的网络结构。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "welsh-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.01\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "模型上下文的保存\n",
    "为了防止在模型训练时，突然发生不可预料的错误（如服务器停电等），我们一般会定时对训练的模型进行保存。\n",
    "如果发生了不可预料的情况，我们就可以从该时间点开始继续训练模型，进而避免重新开始训练模型所造成的高额成本。\n",
    "\n",
    "当然，为了能够更好的恢复训练现场，我们除了保存该时间点下的模型参数之外，还可能需要保存一下模型训练时的变量（即模型上下文），\n",
    "比如已迭代次数和当前学习率等变量。\n",
    "\n",
    "为了方便讲解，让我们来定义一个学习率和优化器：\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "every-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "我们可以将模型参数、优化器参数、迭代次数等都封装到一个字典中：\n",
    "'''\n",
    "checkpoint = {\n",
    "    \"epoch\": 90,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optim_state\": optimizer.state_dict()\n",
    "}\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reverse-parameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存成功\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "同样，我们需要使用 torch.save(checkpoint, FILE) 保存该时间点的模型上下文：\n",
    "'''\n",
    "FILE = \"./model/tmpcheckpoint.pth\"\n",
    "torch.save(checkpoint, FILE)\n",
    "print(\"保存成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "common-incidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "接下来，让我们加载保存在本地的时间点。\n",
    "\n",
    "由于这里保存的仅仅只是参数。因此，我们在加载本地文件之前，都需要指定网络结构和优化器类型：\n",
    "'''\n",
    "model = Model(n_input_features=6)\n",
    "# 这里先将学习率设置为 0，观察加载模型后，是否发生变化\n",
    "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=0)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "durable-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "优化器内参数： {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n",
      "epoch: 90\n"
     ]
    }
   ],
   "source": [
    "#我们还是利用 torch.load(FILE) 加载模型参数、优化器参数、迭代次数等变量：\n",
    "checkpoint = torch.load(FILE)\n",
    "\n",
    "# 加载的文件是一个字典，根据key值，将其加载到模型、优化器、迭次次数中\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "optimizer.load_state_dict(checkpoint['optim_state'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"优化器内参数：\", optimizer.state_dict())\n",
    "print(\"epoch:\", epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
